import requests, faiss, pickle, os, fitz, numpy as np, time
from sentence_transformers import SentenceTransformer

# 1. –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è 8GB VRAM —Ç–≤–æ–µ–π –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã
model = SentenceTransformer("intfloat/multilingual-e5-small")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –≤ Windows
INDEX_FILE = "index.faiss"
DOCS_FILE = "docs.pkl"
NOTES_DIR = "my_notes"

def extract_text(path):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF-–∫–æ–¥–µ–∫—Å–æ–≤."""
    text = ""
    try:
        with fitz.open(path) as doc:
            for page in doc: text += page.get_text()
    except Exception as e: 
        print(f"‚ùå –û—à–∏–±–∫–∞ PDF {path}: {e}")
    return text

def ask_ollama(q, ctx):
    """–ó–∞–ø—Ä–æ—Å –∫ Qwen 2.5 —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
    prompt = (
        f"### –†–û–õ–¨: –¢—ã ‚Äî –ø–µ–¥–∞–Ω—Ç–∏—á–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π —é—Ä–∏—Å—Ç. –¢–≤–æ—è –±–∞–∑–∞ ‚Äî –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n"
        f"### –ü–†–ê–í–ò–õ–ê:\n"
        f"1. –¶–ò–¢–ò–†–£–ô –î–û–°–õ–û–í–ù–û. –ù–µ –º–µ–Ω—è–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏.\n"
        f"2. –í—ã–¥–µ–ª—è–π **–ñ–ò–†–ù–´–ú** –∫–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, **—É—á–∞—Å—Ç–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞**).\n"
        f"3. –ü–∏—à–∏ –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç, –ø–∏—à–∏ '–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ù–ï –ù–ê–ô–î–ï–ù–ê'.\n\n"
        f"### –ö–û–ù–¢–ï–ö–°–¢:\n{ctx}\n\n"
        f"### –í–û–ü–†–û–°: {user_q}\n\n"
        f"### –Æ–†–ò–î–ò–ß–ï–°–ö–ò–ô –û–¢–í–ï–¢:"
    )
    
    start_gen = time.time()
    try:
        r = requests.post("http://localhost:11434/api/generate", 
            json={
                "model": "qwen2.5", 
                "prompt": prompt, 
                "stream": False, 
                "options": {
                    "num_ctx": 8192,     # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —É–º–µ–Ω—å—à–∏–ª–∏ —Å 16k –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    "temperature": 0.0, 
                    "num_predict": 500   # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞
                }
            })
        ans = r.json().get("response", "–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏")
        duration = time.time() - start_gen
        return ans, duration
    except Exception as e:
        return f"–û–®–ò–ë–ö–ê: Ollama –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç. ({e})", 0

if __name__ == "__main__":
    print(f"üìç –†–ê–ë–û–ß–ê–Ø –ü–ê–ü–ö–ê: {os.getcwd()}") # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —ç—Ç–æ C:\cyber_win
    
    loaded = False
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–æ—Ç–æ–≤—É—é –±–∞–∑—É
    if os.path.exists(INDEX_FILE) and os.path.exists(DOCS_FILE):
        try:
            print("‚ö° –ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑—É...")
            index = faiss.read_index(INDEX_FILE)
            with open(DOCS_FILE, "rb") as f:
                chunks = pickle.load(f)
            print(f"‚úÖ –ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞. –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(chunks)}")
            loaded = True
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø–µ—Ä–µ—Å–æ–∑–¥–∞—é): {e}")

    if not loaded:
        # –ü–µ—Ä–≤–∏—á–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        print("‚è≥ –ù–∞—á–∏–Ω–∞—é –ø–µ—Ä–≤–∏—á–Ω—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (–æ–∫–æ–ª–æ 1.5 –º–∏–Ω)...")
        if not os.path.exists(NOTES_DIR) or not os.listdir(NOTES_DIR):
            print(f"‚ùå –û–®–ò–ë–ö–ê: –ü–æ–ª–æ–∂–∏ PDF –≤ –ø–∞–ø–∫—É {NOTES_DIR}!")
            exit()

        chunks = []
        for f_name in os.listdir(NOTES_DIR):
            if f_name.lower().endswith(".pdf"):
                path = os.path.join(NOTES_DIR, f_name)
                raw_text = extract_text(path)
                if raw_text:
                    print(f"üìñ –û–±—Ä–∞–±–æ—Ç–∫–∞ {f_name}...")
                    for i in range(0, len(raw_text), 1100): 
                        chunks.append(f"–§–∞–π–ª: {f_name} | passage: {raw_text[i:i+1500]}")

        if chunks:
            embs = model.encode(chunks)
            index = faiss.IndexFlatL2(embs.shape[1])
            index.add(np.array(embs).astype("float32"))
            faiss.write_index(index, INDEX_FILE)
            with open(DOCS_FILE, "wb") as f:
                pickle.dump(chunks, f)
            print(f"üöÄ –ë–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")

    # –¶–∏–∫–ª –∑–∞–ø—Ä–æ—Å–æ–≤
    while True:
        user_q = input("\nüîé –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ")
        if user_q.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit']: break
        
        # 1. –ü–æ–∏—Å–∫ (–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: k=7 –≤–º–µ—Å—Ç–æ 10)
        start_search = time.time()
        v = model.encode(["query: " + user_q])
        _, ids = index.search(np.array(v).astype("float32"), 7) 
        ctx = "\n---\n".join([chunks[i] for i in ids[0]])
        search_time = time.time() - start_search
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        print(f"‚è≥ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {search_time:.2f} —Å–µ–∫. –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç...")
        answer, gen_time = ask_ollama(user_q, ctx)
        
        print("\n‚úÖ –û–¢–í–ï–¢:\n" + answer)
        print(f"\nüìä –¢–∞–π–º–∏–Ω–≥–∏: –ü–æ–∏—Å–∫: {search_time:.2f}—Å | –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {gen_time:.2f}—Å | –ò—Ç–æ–≥–æ: {search_time+gen_time:.2f}—Å")