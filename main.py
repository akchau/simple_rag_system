import requests
import faiss
import pickle
import numpy as np
import os
from sentence_transformers import SentenceTransformer

# 1. –ú–æ–¥–µ–ª—å E5-small: –∏–¥–µ–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è RTX 4060
model = SentenceTransformer("intfloat/multilingual-e5-small")

def get_context(query, k=5): 
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫—É—Å–∫–æ–≤ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ."""
    if not os.path.exists("index.faiss"): return "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"
    index = faiss.read_index("index.faiss")
    with open("docs.pkl", "rb") as f: docs = pickle.load(f)
    
    # –ú–æ–¥–µ–ª—å E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–∞ 'query: ' –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    v = model.encode(["query: " + query])
    dist, idx = index.search(np.array(v).astype("float32"), k)
    return "\n---\n".join([docs[i] for i in idx[0] if i < len(docs)])

def ask_ollama(q, ctx):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Llama 3 —Å –∂–µ—Å—Ç–∫–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π."""
    prompt = (
        f"### –†–û–õ–¨: –¢—ã ‚Äî —Ä–æ–±–æ—Ç-–∞—Ä—Ö–∏–≤–∞—Ä–∏—É—Å –∫–æ–º–ø–∞–Ω–∏–∏ Aethelgard. "
        f"### –ü–†–ê–í–ò–õ–û: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ö–û–ù–¢–ï–ö–°–¢–ê. "
        f"–ï—Å–ª–∏ –≤ –ö–û–ù–¢–ï–ö–°–¢–ï –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ñ–∞–∫—Ç–∞ ‚Äî –æ—Ç–≤–µ—á–∞–π: '–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'. "
        f"–ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∑–Ω–∞–Ω–∏—è –æ –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–ª–∏ –∑–∞–∫–æ–Ω–∞—Ö.\n\n"
        f"### –ö–û–ù–¢–ï–ö–°–¢:\n{ctx}\n\n"
        f"### –í–û–ü–†–û–°: {q}\n\n"
        f"### –û–¢–í–ï–¢ –ù–ê –†–£–°–°–ö–û–ú:"
    )
    
    try:
        r = requests.post("http://localhost:11434/api/generate", 
            json={
                "model": "llama3", 
                "prompt": prompt, 
                "stream": False,
                "options": {
                    "num_ctx": 16384,     # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è 50 –ú–ë –±–∞–∑—ã
                    "temperature": 0.0,    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –∑–∞–ø—Ä–µ—Ç –Ω–∞ —Ñ–∞–Ω—Ç–∞–∑–∏–∏
                    "num_predict": 1000    # –ú–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                }
            })
        return r.json().get("response", "–û—à–∏–±–∫–∞ LLM")
    except Exception as e: 
        return f"–û–®–ò–ë–ö–ê: –ü—Ä–æ–≤–µ—Ä—å, –∑–∞–ø—É—â–µ–Ω–∞ –ª–∏ Ollama! ({e})"

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–ø–∫–∏ —Å –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π
    if not os.path.exists("my_notes"): os.makedirs("my_notes")
    
    # –ü–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–∞—à–∏
    for f in ["index.faiss", "docs.pkl"]:
        if os.path.exists(f): os.remove(f)

    print("‚è≥ –ò–¥–µ—Ç –≥–ª—É–±–æ–∫–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã (50 –ú–ë)...")
    chunks = []
    chunk_size = 800 # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫—É—Å–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ E5
    
    for f_name in os.listdir("my_notes"):
        file_path = os.path.join("my_notes", f_name)
        with open(file_path, "r", encoding="utf-8") as file:
            text = file.read()
            # –ù–∞—Ä–µ–∑–∫–∞ –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞ –Ω–∞ —á–∞—Å—Ç–∏, —á—Ç–æ–±—ã –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
            for i in range(0, len(text), chunk_size):
                chunk = text[i:i + chunk_size]
                # –ú–æ–¥–µ–ª—å E5 —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–∞ 'passage: ' –¥–ª—è —Ö—Ä–∞–Ω–∏–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                chunks.append(f"–§–∞–π–ª: {f_name} | –¢–µ–∫—Å—Ç: {chunk}")
    
    if chunks:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ RTX 4060
        embs = model.encode(chunks)
        index = faiss.IndexFlatL2(embs.shape[1])
        index.add(np.array(embs).astype("float32"))
        faiss.write_index(index, "index.faiss")
        with open("docs.pkl", "wb") as f: pickle.dump(chunks, f)
        print("‚úÖ –ë–ê–ó–ê –ü–†–û–ò–ù–î–ï–ö–°–ò–†–û–í–ê–ù–ê. –°–ò–°–¢–ï–ú–ê –ì–û–¢–û–í–ê.")
    
    while True:
        user_q = input("\nüîé –ó–∞–ø—Ä–æ—Å: ")
        if user_q.lower() in ['exit', '–≤—ã—Ö–æ–¥', 'quit']: break
        
        context = get_context(user_q)
        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –º–æ–∂–Ω–æ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å context, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å, —á—Ç–æ –Ω–∞—à–µ–ª FAISS
        # print(f"DEBUG: –ù–∞–π–¥–µ–Ω–æ –∫—É—Å–∫–æ–≤: {len(context)}") 
        
        answer = ask_ollama(user_q, context)
        print("\n‚úÖ –û–¢–í–ï–¢:\n" + answer)